{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf55fcf6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90154079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2101cee",
   "metadata": {},
   "source": [
    "### stanza: download persian patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2539ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download(\"fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e153c",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line of file Extraction\n",
    "# Normalization \n",
    "# Tokenization\n",
    "# Store tokens in JSON and .csv file\n",
    "\n",
    "nlp = stanza.Pipeline('fa', processors='tokenize')\n",
    "\n",
    "tokens = []\n",
    "\n",
    "number_of_lines = 0\n",
    "\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh.txt\", \"r\", encoding=\"utf-8\") as shahnameh_file:\n",
    "    for line in shahnameh_file:\n",
    "        number_of_lines += 1\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            doc = nlp(line)\n",
    "            # Extract tokens from all sentences in this line\n",
    "            line_tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "            tokens.extend(line_tokens)\n",
    "            \n",
    "            print(f\"Line: {line}\")\n",
    "            print(f\"Tokens: {line_tokens}\\n\")\n",
    "        \n",
    "\n",
    "print(\"\\nTotal tokens extracted:\", len(tokens))\n",
    "\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_tokens.json\", \"w\", encoding = \"utf-8\") as st:\n",
    "    json.dump(tokens, st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8208729",
   "metadata": {},
   "source": [
    "### Counting Words\n",
    "in Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate words count in corpus (tokens)\n",
    "# store words count in JSON file\n",
    "\n",
    "import json\n",
    "\n",
    "# retreive tokens\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_tokens.json\", \"r\", encoding = \"utf-8\") as shahnameh_tokens:\n",
    "    tokens = json.load(shahnameh_tokens)\n",
    "\n",
    "words = list(set(tokens))\n",
    "\n",
    "word_count = {}\n",
    "for word in words:\n",
    "    word_count[word] = tokens.count(word)\n",
    "\n",
    "word_count[\"<s>\"] = number_of_lines\n",
    "word_count[\"</s>\"] = number_of_lines\n",
    "\n",
    "print(number_of_lines)\n",
    "\n",
    "# store words count\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_word_count.json\", \"w\", encoding = \"utf-8\") as word_counf_file:\n",
    "    json.dump(word_count, word_counf_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536226d",
   "metadata": {},
   "source": [
    "Find out number of repetition of given word in Ferdowsi's Shahnameh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive words count\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_word_count.json\", \"r\", encoding = \"utf-8\") as word_count_file:\n",
    "    word_count = json.load(word_count_file)\n",
    "\n",
    "# print(word_count)\n",
    "\n",
    "word = \"رستم\"\n",
    "\n",
    "if word in word_count:\n",
    "    print(f\"{word}:\", word_count[word])\n",
    "else:\n",
    "    print(f\"{word} not found in word count dictionary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060a5af",
   "metadata": {},
   "source": [
    "### Corpus start/End of line Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh.txt\", \"r\", encoding = \"utf-8\") as input, open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh\\\\file\\\\marked_shahnameh.txt\", \"w\", encoding = \"utf-8\") as output:\n",
    "    for line in input:\n",
    "        marked_line = \"<s> \" + line.strip() + \" </s>\"\n",
    "        output.write(marked_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fc07b",
   "metadata": {},
   "source": [
    "Test if the marking work right or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\marked_shahnameh.txt\", \"r\", encoding = \"utf-8\") as marked_shahnameh_file:\n",
    "    n = 0\n",
    "    for line in marked_shahnameh_file:\n",
    "        line_split = line.split()\n",
    "        for i in range(len(line_split)):\n",
    "            print(f\"{i}: {line_split[i]}\")\n",
    "        n += 1\n",
    "        if n == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cd8c7",
   "metadata": {},
   "source": [
    "### v1.4: Bigrams Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = {}\n",
    "\n",
    "# retreive\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\marked_shahnameh.txt\", \"r\", encoding = \"utf-8\") as marked_shahnameh_file:\n",
    "    for line in marked_shahnameh_file:\n",
    "        line_split = line.split()\n",
    "        for i in range(len(line_split)-1):\n",
    "            bigram = line_split[i] + \" \" + line_split[i+1]\n",
    "            if bigram in bigrams:\n",
    "                bigrams[bigram] += 1\n",
    "            else:\n",
    "                bigrams[bigram] = 1 \n",
    "\n",
    "# store bigrams count\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_bigrams_count.json\", \"w\", encoding = \"utf-8\") as bigrams_file:\n",
    "    json.dump(bigrams, bigrams_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40d387",
   "metadata": {},
   "source": [
    "### v1.5: bigrams Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3fa59",
   "metadata": {},
   "source": [
    "##### Laplace-smoothed Bigram:\n",
    "\n",
    "$$P(W_n \\mid W_{n-1}) = \\frac{C(W_{n-1} W_n) + 1}{C(W_{n-1}) + V}$$\n",
    "\n",
    "$V$ : Represents the *size of the vocabulary* (the total number of unique words in the corpus).\n",
    "\n",
    "• add 1 extra to counts of all bigrams, because of applying \"MLE\"(Maximum Likelihood Estimation). <Page 64> on Jurafsky's slides\n",
    "\n",
    "\n",
    "\n",
    "• Cause I extract bigrams on Shahnameh's lines, I'll not have 0 probability in my Probability Dict. Thus I will not apply the above formula on my probability calculation procedure. I'll just calculate this instead:\n",
    "\n",
    "$$P(W_n \\mid W_{n-1}) = \\frac{C(W_{n-1} W_n)}{C(W_{n-1})}$$\n",
    "\n",
    "Then when every where I want to calculate a probability of a given input and I found no probability item in my \"bigram_probability\" dict, I'll put a so much small value manually as non-zero probability.\n",
    "\n",
    "• Because I have marked my corpus by start and close tokens (<s>, </s>), I should add count of theese tokens to my \"shahnameh_word_count\" file manually to calculate probability truely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_bigrams_count.json\", \"r\", encoding = \"utf-8\") as bigrams_file:\n",
    "    bigrams_count = json.load(bigrams_file)\n",
    "\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_word_count.json\", \"r\", encoding = \"utf-8\") as unique_words_count_file:\n",
    "    unique_words_count = json.load(unique_words_count_file)\n",
    "\n",
    "bigrams_probability = {}\n",
    "\n",
    "for bigram in bigrams_count:\n",
    "    first_word = bigram.split()[0]\n",
    "    if first_word in unique_words_count:\n",
    "        probability = bigrams_count[bigram] / unique_words_count[first_word]\n",
    "        bigrams_probability[bigram] = round(probability, 5)\n",
    "        # if probability > 1.0:\n",
    "        #     print(\"*******************\")\n",
    "        #     print(bigrams_count[bigram], unique_words_count[first_word])\n",
    "    else:\n",
    "        print(f\"{first_word} not found in unique words count dictionary\")\n",
    "\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_bigrams_probability.json\", \"w\", encoding = \"utf-8\") as bigrams_probability_file:\n",
    "    json.dump(bigrams_probability, bigrams_probability_file)\n",
    "\n",
    "\n",
    "for i, j in bigrams_probability.items():\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59753233",
   "metadata": {},
   "source": [
    "### v1.6: Calculate the probability of a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2305713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poem = input(\"Enter a line of poem: \\n\")\n",
    "\n",
    "poem = \"رستم بر\"\n",
    "\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_bigrams_probability.json\", \"r\", encoding = \"utf-8\") as bigram_probability_file:\n",
    "    bigram_probability = json.load(bigram_probability_file)\n",
    "\n",
    "poem_split = poem.split()\n",
    "\n",
    "# print(type(bigram_probability[\"به نام\"]))\n",
    "\n",
    "prob = 1.0\n",
    "\n",
    "for i in range(len(poem_split) - 1):\n",
    "    bigram = poem_split[i] + \" \" + poem_split[i + 1]\n",
    "    if bigram in bigram_probability:\n",
    "        prob = prob * bigram_probability[bigram]\n",
    "    else:\n",
    "        print(f\"{bigram} was not found in the bigram probabilities dictionary!\")\n",
    "\n",
    "print(prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ef868",
   "metadata": {},
   "source": [
    "### v1.7: Generate text with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prob_dict ,start_word = \"به\", num_word = 10):\n",
    "    text = start_word + \" \"\n",
    "\n",
    "    base_word = start_word\n",
    "    while(num_word):\n",
    "        word = \"\"\n",
    "        word_prob = 0.0\n",
    "\n",
    "        for bigram in prob_dict:\n",
    "            if bigram.split()[0] == base_word:\n",
    "                if prob_dict[bigram] > word_prob:\n",
    "                    word_prob = prob_dict[bigram]\n",
    "                    word = bigram.split()[1]\n",
    "        base_word = word\n",
    "        text += base_word + \" \"\n",
    "        num_word -= 1\n",
    "\n",
    "\n",
    "    print(text)\n",
    "\n",
    "\n",
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_bigrams_probability.json\", \"r\", encoding = \"utf-8\") as bigram_probability_file:\n",
    "    bigrams_probability = json.load(bigram_probability_file)\n",
    "\n",
    "generate_text(bigrams_probability, \"رستم\", 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8d8d7",
   "metadata": {},
   "source": [
    "### v1.8: Generate a random text  \n",
    "\n",
    "*(in process of implement)* will complete soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"F:\\\\E\\\\code_workspace\\\\Python\\\\NLP\\\\shahnameh-n-gram-model\\\\file\\\\shahnameh_bigrams_probability.json\", \"r\", encoding = \"utf-8\") as bigram_probability_file:\n",
    "    bigrams_probability = json.load(bigram_probability_file)\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
